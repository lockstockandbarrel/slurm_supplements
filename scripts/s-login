#!/bin/bash
##########################################################################################
#@(#) (INTER) :W: basic login using resources allocated via Slurm
##########################################################################################
# TODO: Just a single-cpu job running bash(1) for now
function VERSION(){
cat <<\EOF
PRODUCT:        Slurm tools
PROGRAM:        s-login
DESCRIPTION:    basic login using resources allocated via Slurm
VERSION:        1.0, 2023-12-29
AUTHOR:         John S. Urban
REPORTING BUGS: http://www.urbanjost.altervista.org/
HOME PAGE:      http://www.urbanjost.altervista.org/index.html
LICENSE:        MIT
EOF
}
##########################################################################################
function GETONE(){
scontrol show reservation
RES=$(scontrol show reservations|xargs -n 1|grep '^ReservationName='|sed -e 's/.*=//'|xargs|tr -s ' ' ',')
srun  \
      --reservation=$RES    \
      -t 364-0:0:0          \
      --mem-per-cpu=0       \
      --cpu-bind=no         \
      --mpi=none            \
      -N1-1                 \
      -n1                   \
      --pty                 \
      --verbose             \
      --overcommit          \
      --propagate=NONE      \
      --export=NONE         \
      $OTHER /usr/bin/bash -l
}
##########################################################################################
VARS(){
cat <<\EOF
SLURM_CLUSTER_NAME=urban
SLURM_JOBID=12171
SLURM_JOB_ID=12171
SLURM_JOB_PARTITION=debug,production,express
SLURM_SUBMIT_HOST=mercury
SLURM_SUBMIT_DIR=/home/urbanjs/venus/V600/SLURM/slurmdocs/bin
SLURM_NODELIST=mercury
SLURM_JOB_NODELIST=mercury
SLURM_NODE_ALIASES=(null)
SLURM_JOB_NAME=/bin/bash
SLURM_NNODES=1
SLURM_JOB_NUM_NODES=1
SLURM_JOB_CPUS_PER_NODE=1
SLURM_TASKS_PER_NODE=1
SLURM_MEM_PER_NODE=300
SLURM_OVERCOMMIT=1
EOF
}
##########################################################################################
# main
#
OTHER=$*

source $(dirname $0)/.functions
STANDARDENV
export SHOULD_NOT_BE_SET=SHOULD_NOT_BE_SET
env -i USER=$USER TERM=$TERM $(which salloc) \
        --partition=$(sinfo --noheader --format=%R|paste -d, -s) \
        --chdir=$HOME \
        --comment='Interactive session' \
        --verbose \
        --job-name=$SHELL \
        --mem=300MB \
        --tmp=1000MB \
        --overcommit \
        $SHELL -
##########################################################################################
exit
##########################################################################################
SallocDefaultCommand    = (null)
15. Can tasks be launched with a remote (pseudo) terminal?
You have several ways to do so, the recommended ones are the following:
The simplest method is to make use of srun's --pty option, (e.g. srun --pty bash -i). Srun's --pty option runs task zero in pseudo terminal mode. Bash's -i option instructs it to run in interactive mode (with prompts).
In addition to that method you have the option to define the SallocDefaultCommand to run a task in pseudo terminal mode directly, so you get the prompt just invoking salloc:

SallocDefaultCommand="srun -n1 -N1 --mem-per-cpu=0 --pty --preserve-env --cpu-bind=no --mpi=none $SHELL"

Finally you can make use of X11 feature and run a graphical terminal. (e.g. srun xterm).
##########################################################################################
[command [args...]]

Parallel run options:
  -c, --cpus-per-task=ncpus   number of cpus required per task

      --gres=list             required generic resources
      --gres-flags=opts       flags related to GRES management
  -k, --no-kill               do not kill job on node failure
  -K, --kill-command[=signal] signal to send terminating job
  -L, --licenses=names        required license, comma separated
  -M, --clusters=names        Comma separated list of clusters to issue
                              commands to.  Default is current cluster.
                              Name of 'all' will submit to run on all clusters.
                              NOTE: SlurmDBD must up.
  -n, --ntasks=N              number of processors required
      --nice[=value]          decrease scheduling priority by value
      --ntasks-per-node=n     number of tasks to invoke on each node
  -N, --nodes=N               number of nodes on which to run (N = min[-max])
      --priority=value        set the priority of the job to value
  -q, --qos=qos               quality of service
      --signal=[B:]num[@time] send signal when time limit within time seconds
  -t, --time=minutes          time limit
      --wckey=wckey           wckey to run job under

Constraint options:
      --mincpus=n             minimum number of logical processors (threads) per node
      --reservation=name      allocate resources from named reservation

      --ntasks-per-core=n     number of tasks to invoke on each core
      --ntasks-per-socket=n   number of tasks to invoke on each socket
##########################################################################################
# wipe the environment to prevent exporting variables and process limits like sbatch(1)
#env -i DISPLAY=$DISPLAY salloc  -t 365-0:0:0  --cpus-per-task=1 --mem-per-cpu=1  $OTHER
#env -i DISPLAY=$DISPLAY srun    -t 364-0:0:0 -n1 --pty "/usr/bin/bash  --jobid=$1 --propagate=NONE $*
#env -i DISPLAY=$DISPLAY srun    -t 364-0:0:0 -n1 --pty --propagate=NONE $OTHER /usr/bin/bash -l

#Usage: srun [-N nnodes] [-n ntasks] [-i in] [-o out] [-e err]
#            [-c ncpus] [-r n] [-p partition] [--hold] [-t minutes]
#            [--immediate[=secs]] [--overcommit] [--no-kill]
#            [--oversubscribe] [--label] [--unbuffered] [-m dist] [-J jobname]
#            [--jobid=id] [--verbose] [--slurmd_debug=#] [--gres=list]
#            [-T threads] [-W sec] [--checkpoint=time] [--gres-flags=opts]
#            [--licenses=names] [--clusters=cluster_names]
#            [--restart-dir=dir] [--qos=qos] [--time-min=minutes]
#            [--contiguous] [--mincpus=n] [--mem=MB] [--tmp=MB] [-C list]
#            [--mpi=type] [--account=name] [--dependency=type:jobid]
#            [--kill-on-bad-exit] [--propagate[=rlimits] [--comment=name]
#            [--cpu-bind=...] [--mem-bind=...] [--network=type]
#            [--ntasks-per-node=n] [--ntasks-per-socket=n] [reservation=name]
#            [--ntasks-per-core=n] [--mem-per-cpu=MB] [--preserve-env]
#            [--profile=...]
#            [--mail-type=type] [--mail-user=user] [--nice[=value]]
#            [--prolog=fname] [--epilog=fname]
#            [--task-prolog=fname] [--task-epilog=fname]
#            [--ctrl-comm-ifhn=addr] [--multi-prog] [--mcs-label=mcs]
#            [--cpu-freq=min[-max[:gov]] [--power=flags] [--spread-job]
#            [--switches=max-switches{@max-time-to-wait}] [--reboot]
#            [--core-spec=cores] [--thread-spec=threads]
#            [--bb=burst_buffer_spec] [--bbf=burst_buffer_file]
#            [--bcast=<dest_path>] [--compress[=library]]
#            [--acctg-freq=<datatype>=<interval>] [--delay-boot=mins]
#            [-w hosts...] [-x hosts...] [--use-min-nodes]
#            [--mpi-combine=yes|no] [--pack-group=value]
#            [--cpus-per-gpu=n] [--gpus=n] [--gpu-bind=...] [--gpu-freq=...]
#            [--gpus-per-node=n] [--gpus-per-socket=n]  [--gpus-per-task=n]
#            [--mem-per-gpu=MB]
#            executable [args...]
# ##########################################################################################
#    --export=NONE --propagate=NONE
#    --get-user-env=50L \
#    --x11 \
# # srun -n1 --pty  bash -l $*
# # srun -n1 --pty --x11 bash -l $*
# # if specify jobid join same ctask so see same private /tmp space and see same environment
# # and resources
# # env -i srun -n1 --pty --x11 bash -l --jobid=$1 --propagate=NONE $*
# ##########################################################################################
# [-N numnodes|[min nodes]-[max nodes]]
# [-n num-processors]
# [[-c cpus-per-node]
# [--mincpus=n] [--tmp=MB]
# [-t minutes]
# [--mem-per-cpu=MB] [--mem=MB]
# [--qos=qos]
#               [-r n]
#               [--immediate[=secs]] [--no-kill] [--overcommit] [--oversubscribe]
#               [--verbose] [--gid=group] [--licenses=names]
#               [-C list]
#               [--comment=name]
#               [--nice[=value]]
#               [--kill-command[=signal]] [--spread-job]
#               [--network=type]
#               [--mem-bind=...] [--reservation=name] [--mcs-label=mcs]
#               [--time-min=minutes] [--gres=list] [--gres-flags=opts]
#               [--core-spec=cores] [--thread-spec=threads] [--reboot]
#               [--cpus-per-gpu=n] [--gpus=n] [--gpu-bind=...] [--gpu-freq=...]
#               [--gpus-per-node=n] [--gpus-per-socket=n]  [--gpus-per-task=n]
#               [--mem-per-gpu=MB]
#               [command [args...]]
#
# Usage: srun [-N nnodes] [-n ntasks] [-i in] [-o out] [-e err]
#             [-c ncpus] [-r n] [-p partition] [--hold] [-t minutes]
#              [--immediate[=secs]] [--overcommit] [--no-kill]
#             [--oversubscribe] [--label] [--unbuffered] [-m dist] [-J jobname]
#             [--jobid=id] [--verbose] [--slurmd_debug=#] [--gres=list]
#             [-T threads] [-W sec] [--checkpoint=time] [--gres-flags=opts]
#             [--licenses=names] [--clusters=cluster_names]
#             [--restart-dir=dir] [--qos=qos] [--time-min=minutes]
#             [--contiguous] [--mincpus=n] [--mem=MB] [--tmp=MB] [-C list]
#             [--mpi=type] [--account=name] [--dependency=type:jobid]
#             [--kill-on-bad-exit] [--propagate[=rlimits] [--comment=name]
#             [--cpu-bind=...] [--mem-bind=...] [--network=type]
#             [--ntasks-per-node=n] [--ntasks-per-socket=n] [reservation=name]
#             [--ntasks-per-core=n] [--mem-per-cpu=MB] [--preserve-env]
#             [--profile=...]
#             [--mail-type=type] [--mail-user=user] [--nice[=value]]
#             [--prolog=fname] [--epilog=fname]
#             [--task-prolog=fname] [--task-epilog=fname]
#             [--ctrl-comm-ifhn=addr] [--multi-prog] [--mcs-label=mcs]
#             [--cpu-freq=min[-max[:gov]] [--power=flags] [--spread-job]
#             [--switches=max-switches{@max-time-to-wait}] [--reboot]
#             [--core-spec=cores] [--thread-spec=threads]
#             [--bb=burst_buffer_spec] [--bbf=burst_buffer_file]
#             [--bcast=<dest_path>] [--compress[=library]]
#             [--acctg-freq=<datatype>=<interval>] [--delay-boot=mins]
#             [-w hosts...] [-x hosts...] [--use-min-nodes]
#             [--mpi-combine=yes|no] [--pack-group=value]
#             [--cpus-per-gpu=n] [--gpus=n] [--gpu-bind=...] [--gpu-freq=...]
#             [--gpus-per-node=n] [--gpus-per-socket=n]  [--gpus-per-task=n]
#             [--mem-per-gpu=MB]
#             executable [args...]
# ##########################################################################################
# Interactive
# Command line
#
# The simplest way to connect to a set of resources is to request an
# interactive shell, which can be accomplished with the salloc command. Here
# is a basic example:
#
#  $ salloc -t 60 --cpus-per-task=1 --mem-per-cpu=32gb --partition=interactive
#
# This example allocates an interactive shell session for 60 minutes (-t
# 60), provides one CPU (--cpus-per-task=1) and 32gb of memory to the
# session (--mem-per-cpu=32gb), and designates that the job should run
# on the interactive partition (--partition=interactive). As the second
# line shows, the requested resources were allocated using node01 and the
# interactive session switched to that node, ready for commands. At the
# end of 60 minutes, the session will be terminated, demonstrating why it
# is important to request a suitable amount of time (if you leave off the
# -t flag and do not specify a time, your session will be allocated only
# 5 minutes).
#
# Once your interactive session starts, you will be in your home directory
# and can begin performing work. But, if you wish to run software with a
# GUI, you must explicitly indicate that by adding the --x11 flag:
#
#     $ salloc -t 60 --cpus-per-task=1 --mem-per-cpu=32gb --partition=interactive --x11
#
# salloc is extremely powerful and there are a number of other options
# you can leverage. One of the most useful flags is --ntasks-per-node,
# which will allocate a specific number of computational cores to the
# session. This can be useful when running software that is optimized for
# parallel operations, such as Stata. For instance, the following example
# modifies the previous command to also request 8 cores:
#
#     $ salloc -t 60 -N 1-1 --ntasks-per-node=8 --mem=32gb --partition=interactive
#
# When finished with your session, the exit command will terminate it and return you to the login node.
# ##########################################################################################
# salloc(1)                       Slurm Commands                       salloc(1)
#
# NAME
#        salloc - Obtain a Slurm job allocation (a set of nodes), execute a com
#        mand, and then release the allocation when the command is finished.
#
# SYNOPSIS
#        salloc [OPTIONS(0)...] [ : [OPTIONS(N)...]] [command(0) [args(0)...]]
#
#        Option(s) define multiple jobs in  a  co-scheduled  heterogeneous  job.
#        For more details about heterogeneous jobs see the document
#        https://slurm.schedmd.com/heterogeneous_jobs.html
#
# DESCRIPTION
#        salloc  is  used  to allocate a Slurm job allocation, which is a set of
#        resources (nodes), possibly with some set of constraints  (e.g.  number
#        of  processors  per  node).   When  salloc successfully obtains the re
#        quested allocation, it then runs the command  specified  by  the  user.
#        Finally,  when  the  user  specified command is complete, salloc relin
#        quishes the job allocation.
#
#        If no command is specified,  then  salloc  runs  the user's default shell.

bug:     It actually runs the value of $SHELL
#
# RETURN VALUE
#        If  salloc  is unable to execute the user command, it will return 1 and
#        print errors to stderr. Else if success or if killed  by  signals  HUP,
#        INT, KILL, or QUIT: it will return 0.
#
# COMMAND PATH RESOLUTION
#        If provided, the command is resolved in the following order:
#
#        1.  If  command  starts  with ".", then path is constructed as: current
#        working directory / command
#        2. If command starts with a "/", then path is considered absolute.
#        3. If command can be resolved through PATH. See path_resolution(7).
#        4. If command is in current working directory.
#
#        Current working directory is the calling process working directory  un
#        less  the  --chdir  argument is passed, which will override the current
#        working directory.
#
#        -D, --chdir=<path>
#               Change  directory  to  path before beginning execution. The path
#               can be specified as full path or relative path to the  directory
#               where the command is executed.
#
#        --comment=<string>
#               An arbitrary comment.
#
#        -C, --constraint=<list>
#               Nodes  can  have features assigned to them by the Slurm adminis
#               trator.  Users can specify which of these features are  required
#               by their job using the constraint option. If you are looking for
#               'soft' constraints please see  --prefer  for  more  information.
#               Only  nodes having features matching the job constraints will be
#               used to satisfy the request.  Multiple constraints may be speci
#               fied  with AND, OR, matching OR, resource counts, etc. (some op
#               erators are not supported on all system types).
#
#               NOTE: Changeable features are features defined by a NodeFeatures
#               plugin.
#
#               Supported --constraint options include:
#
#               Single Name
#                      Only nodes which have the specified feature will be used.
#                      For example, --constraint="intel"
#
#               Node Count
#                      A request can specify the number  of  nodes  needed  with
#                      some feature by appending an asterisk and count after the
#                      feature   name.    For   example,    --nodes=16    --con
#                      straint="graphics*4"  indicates  that the job requires 16
#                      nodes and that at least four of those nodes must have the
#                      feature  "graphics."  If requesting more than one feature
#                      and using node  counts,  the  request  must  have  square
#                      brackets surrounding it.
#
#                      NOTE:  This  option is not supported by the helpers Node
#                      Features plugin.  Heterogeneous jobs can be used instead.
#
#               AND    Only nodes with all of specified features will  be  used.
#                      The  ampersand is used for an AND operator.  For example,
#                      --constraint="intel&gpu"
#
#               OR     Only nodes with at least one of specified  features  will
#                      be used.  The vertical bar is used for an OR operator. If
#                      changeable features are not requested, nodes in the allo
#                      cation  can  have different features. For example, salloc
#                      -N2 --constraint="intel|amd" can result in a job  alloca
#                      tion  where  one node has ttroff: <standard input>:669:
#                      warning [p 9, 5.2i, div 'an-div', 0.2i]: can't break line
                       # he intel feature and the other
#                      node has the amd feature.   However,  if  the  expression
#                      contains  a changeable feature, then all OR operators are
#                      automatically treated as Matching OR so that all nodes in
#                      the job allocation have the same set of features. For ex
#                      ample, salloc -N2 --constraint="foo|bar&baz" The  job  is
#                      allocated two nodes where both nodes have foo, or bar and
#                      baz (one or both nodes could have foo, bar, and baz). The
#                      helpers  NodeFeatures  plugin  will find the first set of
#                      node features that matches all nodes in the  job  alloca
#                      tion;  these  features  are set as active features on the
#                      node and passed to RebootProgram (see slurm.conf(5))  and
#                      the  helper  script  (see helpers.conf(5)). In this case,
#                      the helpers plugin uses the first of "foo"  or  "bar,baz"
#                      that match the two nodes in the job allocation.
#
#               Matching OR
#                      If  only  one of a set of possible options should be used
#                      for all allocated nodes, then use the OR operator and en
#                      close  the  options within square brackets.  For example,
#                      --constraint="[rack1|rack2|rack3|rack4]" might be used to
#                      specify that all nodes must be allocated on a single rack
#                      of the cluster, but any of those four racks can be used.
#
#               Multiple Counts
#                      Specific counts of multiple resources may be specified by
#                      using  the  AND operator and enclosing the options within
#                      square      brackets.       For      example,      --con
#                      straint="[rack1*2&rack2*4]" might be used to specify that
#                      two nodes must be allocated from nodes with  the  feature
#                      of  "rack1"  and  four nodes must be allocated from nodes
#                      with the feature "rack2".
#
#                      NOTE: This construct does not support multiple Intel  KNL
#                      NUMA   or   MCDRAM   modes.  For  example,  while  --con
#                      straint="[(knl&quad)*2&(knl&hemi)*4]" is  not  supported,
#                      --constraint="[haswell*2&(knl&hemi)*4]"   is   supported.
#                      Specification of multiple KNL modes requires the use of a
#                      heterogeneous job.
#
#                      NOTE:  This  option is not supported by the helpers Node
#                      Features plugin.
#
#                      NOTE: Multiple Counts can cause jobs to be allocated with
#                      a non-optimal network layout.
#
#               Brackets
#                      Brackets can be used to indicate that you are looking for
#                      a set of nodes with the different requirements  contained
#                      within     the     brackets.    For    example,    --con
#                      straint="[(rack1|rack2)*1&(rack3)*2]" will  get  you  one
#                      node  with either the "rack1" or "rack2" features and two
#                      nodes with the "rack3" feature.  If requesting more  than
#                      one  feature and using node counts, the request must have
#                      square brackets surrounding it.
#
#                      NOTE: Brackets are only reserved for Multiple Counts  and
#                      Matching  OR  syntax.   AND operators require a count for
#                      each    feature    inside    square    brackets     (i.e.
#                      "[quad*2&hemi*1]"). Slurm will only allow a single set of
#                      bracketed constraints per job.
#
#                      NOTE: Square brackets are not supported  by  the  helpers
#                      NodeFeatures plugin. Matching OR can be requested without
#                      square brackets by using the vertical bar character  with
#                      at least one changeable feature.
#
#               Parentheses
#                      Parentheses  can  be used to group like node features to
#                      gether.           For           example,           --con
#                      straint="[(knl&snc4&flat)*4&haswell*1]"  might be used to
#                      specify that four nodes with the features  "knl",  "snc4"
#                      and  "flat"  plus one node with the feature "haswell" are
#                      required.  Parentheses can also be used to  group  opera
#                      tions.  Without  parentheses,  node  features  are parsed
#                      strictly  from  left  to  right.   For  example,   --con
#                      straint="foo&bar|baz" requests nodes with foo and bar, or
#                      baz.  --constraint="foo|bar&baz" requests nodes with  foo
#                      and  baz, or bar and baz (note how baz was AND'd with ev
#                      erything).  --constraint="foo&(bar|baz)"  requests  nodes
#                      with foo and at least one of bar or baz.  NOTE: OR within
#                      parentheses should not be used with  a  KNL  NodeFeatures
#                      plugin  but  is  supported  by  the  helpers NodeFeatures
#                      plugin.
#
#
#        --cores-per-socket=<cores>
#               Restrict node selection to nodes with  at  least  the  specified
#               number of cores per socket.  See additional information under -B
#               option above when task/affinity plugin is enabled.
#               NOTE: This option may implicitly set the number of tasks (if  -n
#               was not specified) as one task per requested thread.
#
#
#        --cpus-per-gpu=<ncpus>
#               Request that ncpus processors be allocated  per  allocated  GPU.
#               Steps  inheriting this value will imply --exact.  Not compatible
#               with the --cpus-per-task option.
#
#        -c, --cpus-per-task=<ncpus>
#               Advise Slurm that ensuing job steps will require  ncpus  proces
#               sors  per task. By default Slurm will allocate one processor per
#               task.
#
#               For instance, consider an application that has 4 tasks, each re
#               quiring  3 processors.  If our cluster is comprised of quad-pro
#               cessors nodes and we simply ask  for  12  processors,  the  con
#               troller  might  give  us  only  3  nodes.  However, by using the
#               --cpus-per-task=3 options, the controller knows that  each  task
#               requires  3 processors on the same node, and the controller will
#               grant an allocation of 4 nodes, one for each of the 4 tasks.
#
#
#        --exclusive[={user|mcs}]
#               The  job  allocation can not share nodes with other running jobs
#               (or just other users with the "=user" option or with the  "=mcs"
#               option).  If user/mcs are not specified (i.e. the job allocation
#               can not share nodes with other running jobs), the job  is  allo
#               cated  all  CPUs and GRES on all nodes in the allocation, but is
#               only allocated as much memory as it requested. This is by design
#               to  support gang scheduling, because suspended jobs still reside
#               in memory. To request all the memory on  a  node,  use  --mem=0.
#               The default shared/exclusive behavior depends on system configu
#               ration and the partition's OverSubscribe option takes precedence
#               over  the job's option.  NOTE: Since shared GRES (MPS) cannot be
#               allocated at the same time as a sharing GRES (GPU)  this  option
#               only allocates all sharing GRES and no underlying shared GRES.
#
#               NOTE: This option is mutually exclusive with --oversubscribe.
#
#        --get-user-env[=timeout][mode]
#               This  option  will load login environment variables for the user
#               specified in the --uid option.  The  environment  variables  are
#               retrieved  by  running something along the lines of "su - <user
#               name> -c /usr/bin/env" and parsing the output.   Be  aware  that
#               any  environment  variables  already set in salloc's environment
#               will take precedence  over  any  environment  variables  in  the
#               user's login environment.  The optional timeout value is in sec
#               onds. Default value is 3 seconds.  The optional mode value  con
#               trols  the "su" options.  With a mode value of "S", "su" is exe
#               cuted without the "-" option.  With a mode value of "L", "su" is
#               executed with the "-" option, replicating the login environment.
#               If mode is not specified, the mode established  at  Slurm  build
#               time   is  used.   Examples  of  use  include  "--get-user-env",
#               "--get-user-env=10"          "--get-user-env=10L",           and
#               "--get-user-env=S".   NOTE: This option only works if the caller
#               has an effective uid of "root".
#
#        --hint=<type>
#               Bind tasks according to application hints.
#               NOTE:  This  option  cannot  be   used   in   conjunction   with
#               --ntasks-per-core, --threads-per-core or -B. If --hint is speci
#               fied as a command line argument, it will  take  precedence  over
#               the environment.
#
#               compute_bound
#                      Select  settings  for compute bound applications: use all
#                      cores in each socket, one thread per core.
#
#               memory_bound
#                      Select settings for memory bound applications:  use  only
#                      one core in each socket, one thread per core.
#
#               [no]multithread
#                      [don't]  use  extra  threads with in-core multi-threading
#                      which can benefit communication  intensive  applications.
#                      Only supported with the task/affinity plugin.
#
#               help   show this help message
#
#        -L, --licenses=<license>[@db][:count][,license[@db][:count]...]
#               Specification  of  licenses (or other resources available on all
#               nodes of the cluster) which must be allocated to this job.   Li
#               cense  names  can  be followed by a colon and count (the default
#               count is one).  Multiple license names should be comma separated
#               (e.g.  "--licenses=foo:4,bar").
#
#               NOTE:  When  submitting heterogeneous jobs, license requests may
#               only be made on the first component job.  For example "salloc -L
#               ansys:2 :".
#
#        --mem=<size>[units]
#               Specify  the  real  memory required per node.  Default units are
#               megabytes.  Different units can be specified  using  the  suffix
#               [K|M|G|T].  Default value is DefMemPerNode and the maximum value
#               is MaxMemPerNode. If configured, both of parameters can be  seen
#               using  the  scontrol  show config command.  This parameter would
#               generally be used if whole nodes are allocated to jobs  (Select
#               Type=select/linear).   Also see --mem-per-cpu and --mem-per-gpu.
#               The --mem, --mem-per-cpu and --mem-per-gpu options are  mutually
#               exclusive.  If  --mem, --mem-per-cpu or --mem-per-gpu are speci
#               fied as command line arguments, then they will  take  precedence
#               over the environment.
#
#               NOTE:  A  memory size specification of zero is treated as a spe
#               cial case and grants the job access to all of the memory on each
#               node.
#
#               NOTE: Memory requests will not be strictly enforced unless Slurm
#               is configured to use an enforcement  mechanism.  See  Constrain
#               RAMSpace  in  the  cgroup.conf(5) man page and OverMemoryKill in
#               the slurm.conf(5) man page for more details.
#
#
#        --mem-per-cpu=<size>[units]
#               Minimum memory required per usable allocated CPU.  Default units
#               are megabytes.  Different units can be specified using the  suf
#               fix  [K|M|G|T].  The default value is DefMemPerCPU and the maxi
#               mum value is MaxMemPerCPU (see exception below). If  configured,
#               both  parameters can be seen using the scontrol show config com
#               mand.  Note that if the job's --mem-per-cpu  value  exceeds  the
#               configured  MaxMemPerCPU,  then the user's limit will be treated
#               as a memory limit per task; --mem-per-cpu will be reduced  to  a
#               value  no  larger than MaxMemPerCPU; --cpus-per-task will be set
#               and  the  value  of  --cpus-per-task  multiplied  by   the   new
#               --mem-per-cpu  value will equal the original --mem-per-cpu value
#               specified by the user.  This parameter would generally  be  used
#               if  individual  processors are allocated to jobs (SelectType=se
#               lect/cons_tres).  If resources are allocated by core, socket, or
#               whole  nodes,  then the number of CPUs allocated to a job may be
#               higher than the task count and the value of --mem-per-cpu should
#               be adjusted accordingly.  Also see --mem and --mem-per-gpu.  The
#               --mem, --mem-per-cpu and --mem-per-gpu options are mutually  ex
#               clusive.
#
#               NOTE:  If the final amount of memory requested by a job can't be
#               satisfied by any of the nodes configured in the  partition,  the
#               job  will  be  rejected.   This could happen if --mem-per-cpu is
#               used with the  --exclusive  option  for  a  job  allocation  and
#               --mem-per-cpu times the number of CPUs on a node is greater than
#               the total memory of that node.
#
#               NOTE: This applies to usable allocated CPUs in a job allocation.
#               This  is important when more than one thread per core is config
#               ured.  If a job requests --threads-per-core with  fewer  threads
#               on  a core than exist on the core (or --hint=nomultithread which
#               implies --threads-per-core=1), the job will  be  unable  to  use
#               those  extra  threads  on the core and those threads will not be
#               included in the memory per CPU calculation. But if the  job  has
#               access  to  all  threads  on the core, those threads will be in
#               cluded in the memory per CPU calculation even if the job did not
#               explicitly request those threads.
#
#               In the following examples, each core has two threads.
#
#               In  this  first  example,  two  tasks can run on separate hyper
#               threads in the same core because --threads-per-core is not used.
#               The  third  task uses both threads of the second core. The allo
#               cated memory per cpu includes all threads:
#
#               $ salloc -n3 --mem-per-cpu=100
#               salloc: Granted job allocation 17199
#               $ sacct -j $SLURM_JOB_ID -X -o jobid%7,reqtres%35,alloctres%35
#                 JobID                             ReqTRES                           AllocTRES
#               ------- ----------------------------------- -----------------------------------
#                 17199     billing=3,cpu=3,mem=300M,node=1     billing=4,cpu=4,mem=400M,node=1
#
#               In this second example, because  of  --threads-per-core=1,  each
#               task  is  allocated  an  entire core but is only able to use one
#               thread per core. Allocated CPUs includes  all  threads  on  each
#               core. However, allocated memory per cpu includes only the usable
#               thread in each core.
#
#               $ salloc -n3 --mem-per-cpu=100 --threads-per-core=1
#               salloc: Granted job allocation 17200
#               $ sacct -j $SLURM_JOB_ID -X -o jobid%7,reqtres%35,alloctres%35
#                 JobID                             ReqTRES                           AllocTRES
#               ------- ----------------------------------- -----------------------------------
#                 17200     billing=3,cpu=3,mem=300M,node=1     billing=6,cpu=6,mem=300M,node=1
#
#        --mincpus=<n>
#               Specify a minimum number of logical cpus/processors per node.
#
#        --nice[=adjustment]
#               Run  the  job with an adjusted scheduling priority within Slurm.
#               With no adjustment value the scheduling priority is decreased by
#               100. A negative nice value increases the priority, otherwise de
#               creases it. The adjustment range is +/- 2147483645. Only  privi
#               leged users can specify a negative adjustment.
#
#        -N, --nodes=<minnodes>[-maxnodes]|<size_string>
#               Request that a minimum of minnodes nodes be  allocated  to  this
#               job.   A maximum node count may also be specified with maxnodes.
#               If only one number is specified, this is used as both the  mini
#               mum  and maximum node count. Node count can be also specified as
#               size_string.   The  size_string  specification  identifies  what
#               nodes  values  should be used.  Multiple values may be specified
#               using a comma separated list or with a step function  by  suffix
#               containing  a colon and number values with a "-" separator.  For
#               example, "--nodes=1-15:4" is equivalent  to  "--nodes=1,5,9,13".
#               The  partition's  node  limits supersede those of the job.  If a
#               job's node limits are outside of the range permitted for its as
#               sociated  partition,  the  job  will be left in a PENDING state.
#               This permits possible execution at a later time, when the parti
#               tion  limit  is changed.  If a job node limit exceeds the number
#               of nodes configured in the partition, the job will be  rejected.
#               Note  that  the environment variable SLURM_JOB_NUM_NODES will be
#               set to the count of nodes actually allocated to the job. See the
#               ENVIRONMENT  VARIABLES   section for more information.  If -N is
#               not specified, the default behavior is to allocate enough  nodes
#               to satisfy the requested resources as expressed by per-job spec
#               ification options, e.g. -n, -c and --gpus.  The job will be  al
#               located as many nodes as possible within the range specified and
#               without delaying the initiation of  the  job.   The  node  count
#               specification  may  include a numeric value followed by a suffix
#               of "k" (multiplies numeric value by 1,024)  or  "m"  (multiplies
#               numeric value by 1,048,576).
#
#               NOTE: This option cannot be used in with arbitrary distribution.
#
#        -n, --ntasks=<number>
#               salloc  does  not launch tasks, it requests an allocation of re
#               sources and executed some command. This option advises the Slurm
#               controller that job steps run within this allocation will launch
#               a maximum of number tasks and sufficient resources are allocated
#               to  accomplish this.  The default is one task per node, but note
#               that the --cpus-per-task option will change this default.
#
#        --ntasks-per-core=<ntasks>
#               Request the maximum ntasks be invoked on each core.  Meant to be
#               used with the --ntasks option.  Related to --ntasks-per-node ex
#               cept at the core level instead of the node  level.  This  option
#               will  be  inhertited by srun.  Slurm may allocate more cpus than
#               what was requested in order to respect this option.
#               NOTE: This option is not  supported  when  using  SelectType=se
#               lect/linear.    This    value    can   not   be   greater   than
#               --threads-per-core.
#
#        --ntasks-per-node=<ntasks>
#               Request  that  ntasks be invoked on each node.  If used with the
#               --ntasks option, the --ntasks option will  take  precedence  and
#               the  --ntasks-per-node  will  be  treated  as a maximum count of
#               tasks per node.  Meant to be used with the --nodes option.  This
#               is related to --cpus-per-task=ncpus, but does not require knowl
#               edge of the actual number of cpus on each node.  In some  cases,
#               it  is more convenient to be able to request that no more than a
#               specific number of tasks be invoked on each node.   Examples  of
#               this  include  submitting a hybrid MPI/OpenMP app where only one
#               MPI "task/rank" should be assigned to each node  while  allowing
#               the  OpenMP portion to utilize all of the parallelism present in
#               the node, or submitting a single setup/cleanup/monitoring job to
#               each  node  of a pre-existing allocation as one step in a larger
#               job script.
#
#        --ntasks-per-socket=<ntasks>
#               Request the maximum ntasks be invoked on each socket.  Meant  to
#               be  used with the --ntasks option.  Related to --ntasks-per-node
#               except at the socket level instead of  the  node  level.   NOTE:
#               This  option  is not supported when using SelectType=select/lin
#               ear.
#
#        -O, --overcommit
#               Overcommit resources.
#
#               When applied to a job allocation (not including jobs  requesting
#               exclusive access to the nodes) the resources are allocated as if
#               only one task per node is requested. This  means  that  the  re
#               quested  number of cpus per task (-c, --cpus-per-task) are allo
#               cated per node rather than being multiplied  by  the  number  of
#               tasks.  Options  used  to  specify the number of tasks per node,
#               socket, core, etc. are ignored.
#
#               When applied to job step allocations (the srun command when exe
#               cuted  within  an  existing  job allocation), this option can be
#               used to launch more than one task per CPU.  Normally, srun  will
#               not  allocate  more  than  one  process  per CPU.  By specifying
#               --overcommit you are explicitly allowing more than  one  process
#               per  CPU. However no more than MAX_TASKS_PER_NODE tasks are per
#               mitted to execute per node.  NOTE: MAX_TASKS_PER_NODE is defined
#               in  the  file  slurm.h and is not a variable, it is set at Slurm
#               build time.
#
#        -s, --oversubscribe
#               The job allocation can over-subscribe resources with other  run
#               ning  jobs.   The  resources to be over-subscribed can be nodes,
#               sockets, cores, and/or hyperthreads  depending  upon  configura
#               tion.   The  default  over-subscribe  behavior depends on system
#               configuration and the  partition's  OverSubscribe  option  takes
#               precedence over the job's option.  This option may result in the
#               allocation being granted sooner than if the --oversubscribe  op
#               tion was not set and allow higher system utilization, but appli
#               cation performance will likely suffer due to competition for re
#               sources.  Also see the --exclusive option.
#
#               NOTE: This option is mutually exclusive with --exclusive.
#
#        -p, --partition=<partition_names>
#               Request  a  specific  partition for the resource allocation.  If
#               not specified, the default behavior is to allow the  slurm  con
#               troller  to  select  the  default partition as designated by the
#               system administrator. If the job can use more  than  one  parti
#               tion,  specify  their names in a comma separate list and the one
#               offering earliest initiation will be used with no  regard  given
#               to  the partition name ordering (although higher priority parti
#               tions will be considered first).  When the job is initiated, the
#               name  of  the  partition  used  will  be placed first in the job
#               record partition string.
#
#
#        --prefer=<list>
#               Nodes  can  have features assigned to them by the Slurm adminis
#               trator.  Users can specify which of these features  are  desired
#               but not required by their job using the prefer option.  This op
#               tion operates independently from --constraint and will  override
#               whatever  is  set  there if possible.  When scheduling, the fea
#               tures in --prefer are tried first. If a node set isn't available
#               with  those features then --constraint is attempted.  See --con
#               straint for more information, this option behaves the same way.
#
#        --priority=<value>
#               Request a specific job priority.  May be subject  to  configura
#               tion  specific  constraints.   value  should either be a numeric
#               value or "TOP" (for highest possible value).  Only Slurm  opera
#               tors and administrators can set the priority of a job.
#
#        -q, --qos=<qos>
#               Request a quality of service for the job.  QOS values can be de
#               fined for each utroff: <standard input>:2231: name expected (got a special character): treated as missing
# ser/cluster/account  association  in  the  Slurm
#               database.   Users will be limited to their association's defined
#               set of qos's when the Slurm  configuration  parameter,  Account
#               ingStorageEnforce, includes "qos" in its definition.
#
#        --reservation=<reservation_names>
#               Allocate  resources  for  the job from the named reservation. If
#               the job can use more than one reservation, specify  their  names
#               in  a  comma separate list and the one offering earliest initia
#               tion. Each reservation will be considered in the  order  it  was
#               requested.   All  reservations will be listed in scontrol/squeue
#               through the life of the job.  In accounting the  first  reserva
#               tion  will be seen and after the job starts the reservation used
#               will replace it.
#
#        --sockets-per-node=<sockets>
#               Restrict node selection to nodes with  at  least  the  specified
#               number  of  sockets.  See additional information under -B option
#               above when task/affinity plugin is enabled.
#               NOTE: This option may implicitly set the number of tasks (if  -n
#               was not specified) as one task per requested thread.
#
#        --threads-per-core=<threads>
#               Restrict  node  selection  to  nodes with at least the specified
#               number of threads per core. In task layout,  use  the  specified
#               maximum  number  of  threads per core. NOTE: "Threads" refers to
#               the number of processing units on each core rather than the num
#               ber  of  application  tasks  to be launched per core.  See addi
#               tional information under  -B  option  above  when  task/affinity
#               plugin is enabled.
#               NOTE:  This option may implicitly set the number of tasks (if -n
#               was not specified) as one task per requested thread.
#
#        -t, --time=<time>
#               Set a limit on the total run time of the job allocation.  If the
#               requested time limit exceeds the partition's time limit, the job
#               will be left in a PENDING state  (possibly  indefinitely).   The
#               default  time limit is the partition's default time limit.  When
#               the time limit is reached, each task in each job  step  is  sent
#               SIGTERM  followed  by  SIGKILL.  The interval between signals is
#               specified by the Slurm configuration  parameter  KillWait.   The
#               OverTimeLimit  configuration parameter may permit the job to run
#               longer than scheduled.  Time resolution is one minute and second
#               values are rounded up to the next minute.
#
#               A  time  limit  of  zero requests that no time limit be imposed.
#               Acceptable time formats  include  "minutes",  "minutes:seconds",
#               "hours:minutes:seconds",  "days-hours", "days-hours:minutes" and
#               "days-hours:minutes:seconds".
#
#        --time-min=<time>
#               Set a minimum time limit on the job allocation.   If  specified,
#               the  job  may  have its --time limit lowered to a value no lower
#               than --time-min if doing so permits the job to  begin  execution
#               earlier  than otherwise possible.  The job's time limit will not
#               be changed after the job is allocated resources.  This  is  per
#               formed  by a backfill scheduling algorithm to allocate resources
#               otherwise reserved for higher priority  jobs.   Acceptable  time
#               formats   include   "minutes",   "minutes:seconds",  "hours:min
#               utes:seconds",    "days-hours",     "days-hours:minutes"     and
#               "days-hours:minutes:seconds".
#
#        --tmp=<size>[units]
#               Specify  a minimum amount of temporary disk space per node.  De
#               fault units are megabytes.  Different units can be specified us
#               ing the suffix [K|M|G|T].
#
#        --tres-bind=<tres>:[verbose,]<type>[+<tres>:
#               [verbose,]<type>...]   Specify  a  list  of tres with their task
#               binding options. Currently gres are the only supported tres  for
#               this options. Specify gres as "gres/<gres_name>" (e.g. gres/gpu)
#
#               Example: --tres-bind=gres/gpu:verbose,map:0,1,2,3+gres/nic:clos
#               est
#
#               By default, most tres are not bound to individual tasks
#
#               Supported binding type options for gres:
#
#               closest   Bind each task to the gres(s) which are closest.  In a
#                         NUMA  environment, each task may be bound to more than
#                         one gres (i.e.  all gres in that NUMA environment).
#
#               map:<list>
#                         Bind by setting gres masks  on  tasks  (or  ranks)  as
#                         specified           where           <list>          is
#                         <gres_id_for_task_0>,<gres_id_for_task_1>,... gres IDs
#                         are  interpreted  as  decimal values. If the number of
#                         tasks (or ranks) exceeds the  number  of  elements  in
#                         this  list,  elements  in  the  list will be reused as
#                         needed starting from the beginning  of  the  list.  To
#                         simplify  support for large task counts, the lists may
#                         follow a map with an asterisk  and  repetition  count.
#                         For  example "map:0*4,1*4".  If the task/cgroup plugin
#                         is used and ConstrainDevices is  set  in  cgroup.conf,
#                         then  the  gres IDs are zero-based indexes relative to
#                         the gress allocated to the job (e.g. the first gres is
#                         0,  even  if  the global ID is 3). Otherwise, the gres
#                         IDs are global IDs, and all gres on each node  in  the
#                         job  should  be  allocated for predictable binding re
#                         sults.
#
#               mask:<list>
#                         Bind by setting gres masks  on  tasks  (or  ranks)  as
#                         specified           where           <list>          is
#                         <gres_mask_for_task_0>,<gres_mask_for_task_1>,...  The
#                         mapping  is specified for a node and identical mapping
#                         is applied to the tasks on every node (i.e. the lowest
#                         task ID on each node is mapped to the first mask spec
#                         ified in the list, etc.). gres masks are always inter
#                         preted  as hexadecimal values but can be preceded with
#                         an optional '0x'. To simplify support for  large  task
#                         counts,  the  lists  may follow a map with an asterisk
#                         and      repetition      count.       For      example
#                         "mask:0x0f*4,0xf0*4".   If  the  task/cgroup plugin is
#                         used and ConstrainDevices is set in cgroup.conf,  then
#                         the  gres  IDs  are zero-based indexes relative to the
#                         gres allocated to the job (e.g. the first gres  is  0,
#                         even  if  the global ID is 3). Otherwise, the gres IDs
#                         are global IDs, and all gres on each node in  the  job
#                         should be allocated for predictable binding results.
#
#               none      Do  not  bind  tasks  to this gres (turns off implicit
#                         binding from --tres-per-task and --gpus-per-task).
#
#               per_task:<gres_per_task>
#                         Each task will be bound to the number of  gres  speci
#                         fied  in <gres_per_task>. Tasks are preferentially as
#                         signed gres with affinity to cores in their allocation
#                         like  in  closest,  though  they will take any gres if
#                         they are unavailable. If no affinity exists, the first
#                         task  will  be  assigned the first x number of gres on
#                         the node etc.  Shared gres will  prefer  to  bind  one
#                         sharing device per task if possible.
#
#               single:<tasks_per_gres>
#                         Like  closest, except that each task can only be bound
#                         to a single gres, even when it can be bound to  multi
#                         ple  gres that are equally close.  The gres to bind to
#                         is determined by  <tasks_per_gres>,  where  the  first
#                         <tasks_per_gres>  tasks  are  bound  to the first gres
#                         available, the second <tasks_per_gres> tasks are bound
#                         to  the second gres available, etc.  This is basically
#                         a block distribution of  tasks  onto  available  gres,
#                         where  the available gres are determined by the socket
#                         affinity of the task and the socket  affinity  of  the
#                         gres as specified in gres.conf's Cores parameter.
#
#                         NOTE:  Shared  gres  binding  is  currently limited to
#                         per_task or none
#
#
#        --use-min-nodes
#               If a range of node counts is given, prefer the smaller count.
#
#        --wckey=<wckey>
#               Specify  wckey  to be used with job.  If TrackWCKey=no (default)
#               in the slurm.conf this value is ignored.
#
#        --x11[={all|first|last}]
#               Sets up X11 forwarding on "all", "first" or  "last"  node(s)  of
#               the  allocation.   This option is only enabled if Slurm was com
#               piled with X11 support and PrologFlags=x11  is  defined  in  the
#               slurm.conf. Default is "all".
#
# INPUT ENVIRONMENT VARIABLES
#        Upon startup, salloc will read and handle the options set in  the  fol
#        lowing  environment  variables. The majority of these variables are set
#        the same way the options are set, as defined above.  For  flag  options
#        that  are  defined  to expect no argument, the option can be enabled by
#        setting the  environment  variable  without  a  value  (empty  or  NULL
#        string),  the  string  'yes', or a non-zero number. Any other value for
#        the environment variable will result  in  the  option  not  being  set.
#        There are a couple exceptions to these rules that are noted below.
#        NOTE:  Command  line options always override environment variables set
#        tings.
#
#        SALLOC_ACCOUNT        Same as -A, --account
#        SALLOC_CLUSTERS or SLURM_CLUSTERS Same as --clusters
#        SALLOC_CONSTRAINT     Same as -C, --constraint
#        SALLOC_CPUS_PER_GPU   Same as --cpus-per-gpu
#        SALLOC_EXCLUSIVE      Same as --exclusive
#        SALLOC_GRES           Same as --gres
#        SALLOC_GRES_FLAGS     Same as --gres-flags
#        SALLOC_HINT or SLURM_HINT Same as --hint
#        SALLOC_KILL_CMD       Same as -K, --kill-command
#        SALLOC_MEM_BIND       Same as --mem-bind
#        SALLOC_MEM_PER_CPU    Same as --mem-per-cpu
#        SALLOC_MEM_PER_GPU    Same as --mem-per-gpu
#        SALLOC_MEM_PER_NODE   Same as --mem
#        SALLOC_NETWORK        Same as --network
#        SALLOC_NO_BELL        Same as --no-bell
#        SALLOC_NO_KILL        Same as -k, --no-kill
#        SALLOC_OVERCOMMIT     Same as -O, --overcommit
#        SALLOC_PARTITION      Same as -p, --partition
#        SALLOC_POWER          Same as --power
#        SALLOC_PROFILE        Same as --profile
#        SALLOC_QOS            Same as --qos
#        SALLOC_RESERVATION    Same as --reservation
#        SALLOC_SIGNAL         Same as --signal
#        SALLOC_SPREAD_JOB     Same as --spread-job
#        SALLOC_THREAD_SPEC    Same as --thread-spec
#        SALLOC_THREADS_PER_CORE Same as --threads-per-core
#        SALLOC_TIMELIMIT      Same as -t, --time
#        SALLOC_TRES_BIND      Same as --tres-bind
#        SALLOC_TRES_PER_TASK  Same as --tres-per-task
#        SALLOC_USE_MIN_NODES  Same as --use-min-nodes
#        SALLOC_WAIT_ALL_NODES Same as --wait-all-nodes. Must be set to 0  or  1 to disable or enable the option.
#        SALLOC_WAIT4SWITCH    Max  time  waiting  for  requested  switches. See --switches
#        SALLOC_WCKEY          Same as --wckey
#        SLURM_CONF            The location of the Slurm configuration file.
#        SLURM_EXIT_ERROR      Specifies  the  exit  code generated when a Slurm
#                              error occurs (e.g. invalid options).  This can be
#                              used  by a script to distinguish application exit
#                              codes from various Slurm error conditions.   Also
#                              see SLURM_EXIT_IMMEDIATE.
#
# EXAMPLES
#        To  get  an allocation, and open a new xterm in which srun commands may
#        be typed interactively:
#
#               $ salloc -N16 xterm
#               salloc: Granted job allocation 65537
#               # (at this point the xterm appears, and salloc waits for xterm to exit)
#               salloc: Relinquishing job allocation 65537
#
#        To grab an allocation of nodes and launch a parallel application on one
#        command line:
#
#               $ salloc -N5 srun -n10 myprogram
#
#        To  create  a  heterogeneous  job  with 3 components, each allocating a
#        unique set of nodes:
#
#               $ salloc -w node[2-3] : -w node4 : -w node[5-7] bash
#               salloc: job 32294 queued and waiting for resources
#               salloc: job 32294 has been allocated resources
#               salloc: Granted job allocation 32294
#

slurm-users
Privacy
     
 Terms
 9 of 8019
 [slurm-users] salloc+srun vs just srun
 6 views
 wdennis--- via slurm-users's profile photo
 wdennis--- via slurm-users
 unread,
 Feb 28, 2024, 10:19:15   AM (3 days ago) 
 to slurm...@lists.schedmd.com
 Hi list,

 In our institution, our instructions to users who want to spawn an interactive job (for us, a bash shell) have always been to do "srun ..." from the login node, which has always been working well for us. But when we had a recent Slurm training, the SchedMD folks advised us to use "salloc" and then "srun" to do interactive jobs. I tried this today, "salloc" gave me a shell on a server, the same as srun does, but then when I tried to "srun [programname]" it hung there with no output. Of course when I tried "srun [programname] &" it spawned the background job, and gave me back a prompt. Either time I had to Ctrl-C the running srun job, and got no output other than the srun/slurmstepd termination output.

 I think I read somewhere that directly invoking srun creates an allocation; why then would I want to do an initial salloc, and then srun? (i the case that I want a foreground program, such as a bash shell)

 I have surveyed some other institution's Slurm interactive jobs documentation for users, I see both examples of advice to run srun directly, or salloc and then srun.

 Please help me to understand how this is intended to work, and if we are "doing it wrong" :)

 Thanks,
 Will

 -- 
 slurm-users mailing list -- slurm...@lists.schedmd.com
 To unsubscribe send an email to slurm-us...@lists.schedmd.com
 Paul Edmon via slurm-users's profile photo
 Paul Edmon via slurm-users
 unread,
 Feb 28, 2024, 10:26:58   AM (3 days ago) 
 to slurm...@lists.schedmd.com
 salloc is the currently recommended way for interactive sessions. srun
 is now intended for launching steps or MPI applications. So properly you
 would salloc and then srun inside the salloc.

 As you've noticed with srun you tend lose control of your shell as it
 takes over so you have background the process unless it is the main
 process. We've hit this before when people use srun to subschedule in a
 salloc.

 You can also just launch the salloc and then operate via the normal
 command line reserving srun for things like launching MPI.

 The reason they changed from srun to salloc is that you can't srun
 inside a srun. So if you were a user who started a srun interactive
 session and then you tried to invoke MPI it would get weird as you would
 be invoking another srun. By using salloc you avoid this issue.

 We used to use srun for interactive sessions as well but swapped to
 salloc a few years back and haven't had any issues.

 -Paul Edmon-
 Paul Raines via slurm-users's profile photo
 Paul Raines via slurm-users
 unread,
 Feb 28, 2024, 10:48:08   AM (3 days ago) 
 to Paul Edmon, slurm...@lists.schedmd.com

 What do you mean "operate via the normal command line"? When
 you salloc, you are still on the login node.

 $ salloc -p rtx6000 -A sysadm -N 1 --ntasks-per-node=1 --mem=20G
 --time=1-10:00:00 --gpus=2 --cpus-per-task=2 /bin/bash
 salloc: Pending job allocation 3798364
 salloc: job 3798364 queued and waiting for resources
 salloc: job 3798364 has been allocated resources
 salloc: Granted job allocation 3798364
 salloc: Waiting for resource configuration
 salloc: Nodes rtx-02 are ready for job
 mesg: cannot open /dev/pts/91: Permission denied
 mlsc-login[0]:~$ hostname
 mlsc-login.nmr.mgh.harvard.edu
 mlsc-login[0]:~$ printenv | grep SLURM_JOB_NODELIST
 SLURM_JOB_NODELIST=rtx-02

 Seems you MUST use srun

 -- Paul Raines (http://help.nmr.mgh.harvard.edu)

 On Wed, 28 Feb 2024 10:25am, Paul Edmon via slurm-users wrote:

 Paul Edmon via slurm-users
 
 Feb 28, 2024, 10:50:16   AM (3 days ago) 
 to Paul Raines, slurm...@lists.schedmd.com
 He's talking about recent versions of Slurm which now have this option:

 https://slurm.schedmd.com/slurm.conf.html#OPT_use_interactive_step

 -Paul Edmon-
 wdennis--- via slurm-users's profile photo
 wdennis--- via slurm-users
 unread,
 Feb 29, 2024, 12:27:29   AM (2 days ago) 
 to slurm...@lists.schedmd.com
 Thanks for the logical explanation, Paul. So when I rewrite my user documentation, I'll mention using `salloc` instead of `srun`.

 Yes, we do have `LaunchParameters=use_interactive_step` set on our cluster, so salloc gives a shell on the allocated host.

 Best,


lists.schedmd.com

    Sign In
    Sign Up

    Manage this list


canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?
salloc+srun vs just srun
Enforcing relative resource...
Josef Dvoracek
26 Feb 2024 3:27 a.m.

What is the recommended way to run longer interactive job at your systems?

Our how-to includes starting screen at front-end node and running srun 
with bash/zsh inside,
but that indeed brings dependency between login node (with screen) and 
the compute node job.

On systems with multiple front-ends users need to remember the login 
node where they have their screen session..

Are you anybody using something more advanced and still understandable 
by casual user of HPC?

(I know Open On Demand, but often the use of native console has certain 
benefits. )

cheers

josef

Attachments:

    smime.p7s (application/pkcs7-signature — 4.2 KB)

0 0
Reply

Show replies by date
Ward Poelmans
26 Feb 3:47 a.m.

Hi,

On 26/02/2024 09:27, Josef Dvoracek via slurm-users wrote:
...

    Are you anybody using something more advanced and still understandable by casual user of HPC?

I'm not sure it qualifies but:

sbatch --wrap 'screen -D -m'
srun --jobid <PREV JOB_ID> --pty screen -rd

Or:
sbatch -J screen --wrap 'screen -D -m'
srun --jobid $(squeue -n screen -h -o '%A') --pty screen -rd

Ward
0 0
Reply attachment
Josef Dvoracek
28 Feb 8:30 a.m.

From unclear reason "--wrap" was not part of my /repertoire/ so far.

thanks

On 26. 02. 24 9:47, Ward Poelmans via slurm-users wrote:
...

    sbatch --wrap 'screen -D -m'
    srun --jobid <PREV JOB_ID> --pty screen -rd

0 0
Reply attachments
Brian Andrus
27 Feb 11:21 a.m.

Josef,

for us, we put a load balancer in front of the login nodes with session 
affinity enabled. This makes them land on the same backend node each time.

Also, for interactive X sessions, users start a desktop session on the 
node and then use vnc to connect there. This accommodates disconnection 
for any reason even for X-based apps.

Personally, I don't care much for interactive sessions in HPC, but there 
is a large body that only knows how to do things that way, so it is there.

Brian Andrus

On 2/26/2024 12:27 AM, Josef Dvoracek via slurm-users wrote:
...
0 0
Reply
Hagdorn, Magnus Karl Moritz
28 Feb 3:10 a.m.
New subject: [ext] Re: canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?

On Tue, 2024-02-27 at 08:21 -0800, Brian Andrus via slurm-users wrote:
...

Hi Brian,
that sounds interesting - how did you implement session affinity?
cheers
magnus
-- 
Magnus Hagdorn
Charité – Universitätsmedizin Berlin
Geschäftsbereich IT | Scientific Computing
 
Campus Charité Mitte
BALTIC - Invalidenstraße 120/121
10115 Berlin
 
magnus.hagdorn@charite.de
https://www.charite.de
HPC Helpdesk: sc-hpc-helpdesk@charite.de
0 0
Reply attachment
Brian Andrus
3:36 p.m.
New subject: [ext] Re: canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?

Magnus,

That is a feature of the load balancer. Most of them have that these days.

Brian Andrus

On 2/28/2024 12:10 AM, Hagdorn, Magnus Karl Moritz via slurm-users wrote:
...
0 0
Reply
Dan Healy
3:54 p.m.
New subject: [ext] Re: canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?

Are most of us using HAProxy or something else?

On Wed, Feb 28, 2024 at 3:38 PM Brian Andrus via slurm-users <
slurm-users@lists.schedmd.com> wrote:
...
-- 
Thanks,

Daniel Healy


0 0
Reply attachment
Cutts, Tim
28 Feb 4:29 p.m.
New subject: [ext] Re: canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?

HAProxy, for on-prem things.  In the cloud I just use their load balancers rather than implement my own.

Tim

--
Tim Cutts
Scientific Computing Platform Lead
AstraZeneca

Find out more about R&D IT Data, Analytics & AI and how we can support you by visiting our Service Cataloguehttps://azcollaboration.sharepoint.com/sites/CMU993 |

From: Dan Healy via slurm-users slurm-users@lists.schedmd.com
Date: Wednesday, 28 February 2024 at 20:56
To: Brian Andrus toomuchit@gmail.com
Cc: slurm-users@lists.schedmd.com slurm-users@lists.schedmd.com
Subject: [slurm-users] Re: [ext] Re: canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?
Are most of us using HAProxy or something else?

On Wed, Feb 28, 2024 at 3:38 PM Brian Andrus via slurm-users <slurm-users@lists.schedmd.commailto:slurm-users@lists.schedmd.com> wrote:
Magnus,

That is a feature of the load balancer. Most of them have that these days.

Brian Andrus

On 2/28/2024 12:10 AM, Hagdorn, Magnus Karl Moritz via slurm-users wrote:
...

--
slurm-users mailing list -- slurm-users@lists.schedmd.commailto:slurm-users@lists.schedmd.com
To unsubscribe send an email to slurm-users-leave@lists.schedmd.commailto:slurm-users-leave@lists.schedmd.com

--
Thanks,

Daniel Healy
________________________________

AstraZeneca UK Limited is a company incorporated in England and Wales with registered number:03674842 and its registered office at 1 Francis Crick Avenue, Cambridge Biomedical Campus, Cambridge, CB2 0AA.

This e-mail and its attachments are intended for the above named recipient only and may contain confidential and privileged information. If they have come to you in error, you must not copy or show them to anyone; instead, please reply to this e-mail, highlighting the error to the sender and then immediately delete the message. For information about how AstraZeneca UK Limited and its affiliates may process information, personal data and monitor communications, please see our privacy notice at www.astrazeneca.comhttps://www.astrazeneca.com
0 0
Reply attachment
Brian Andrus
5:04 p.m.
New subject: [ext] Re: canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?

Most of my stuff is in the cloud, so I use their load balancing services.

HAProxy does have sticky sessions, which you can enable based on IP so 
it works with other protocols: 2 Ways to Enable Sticky Sessions in 
HAProxy (Guide) 
https://www.haproxy.com/blog/enable-sticky-sessions-in-haproxy

Brian Andrus

On 2/28/2024 12:54 PM, Dan Healy wrote:
...
0 0
Reply attachment
Chris Samuel
2:18 a.m.

On 26/2/24 12:27 am, Josef Dvoracek via slurm-users wrote:
...

    What is the recommended way to run longer interactive job at your systems?

We provide NX for our users and also access via JupyterHub.

We also have high priority QOS's intended for interactive use for rapid 
response, but they are capped at 4 hours (or 6 hours for Jupyter users).

All the best,
Chris
###############################################################################
# TODO: Just a single-cpu job running bash(1) for now
if which screen 2>&1 >>/dev/null; then
   echo 'executing srun(1) using screen(1)'
   exec screen -a -A -R -h 2000 -e'^I^I' -c $(dirname $0)/.screenrc
elif which tmux 2&1 >>/dev/null; then
   echo 'executing srun(1) using tmux(1)'
   exec tmux -f $dirname $0/.tmux.conf -c "srun -N1-1 -n1 --pty  bash -l $*"
else
   echo 'ERROR: neither tmux(1) nor screen(1) available. Session not persistent'
   srun -N1-1 -n1 --pty  bash -l $*
fi
##########################################################################################
exit
##########################################################################################

slurm-users
Privacy
    
Terms
1 of 8011
[slurm-users] canonical way to run longer shell/bash interactive job (instead of srun inside of screen/tmux at front-end)?
0 views
Josef Dvoracek via slurm-users's profile photo
Josef Dvoracek via slurm-users
unread,
3:29   AM (3 hours ago)
to Slurm User Community List
What is the recommended way to run longer interactive job at your systems?

Our how-to includes starting screen at front-end node and running srun
with bash/zsh inside,
but that indeed brings dependency between login node (with screen) and
the compute node job.

On systems with multiple front-ends users need to remember the login
node where they have their screen session..

Are you anybody using something more advanced and still understandable
by casual user of HPC?

(I know Open On Demand, but often the use of native console has certain
benefits. )

cheers

josef

Ward Poelmans via slurm-users's profile photo
Ward Poelmans via slurm-users
unread,
3:49   AM (2 hours ago)
to slurm...@lists.schedmd.com
Hi,

On 26/02/2024 09:27, Josef Dvoracek via slurm-users wrote:

> Are you anybody using something more advanced and still understandable by casual user of HPC?

I'm not sure it qualifies but:

sbatch --wrap 'screen -D -m'
srun --jobid <PREV JOB_ID> --pty screen -rd

Or:
sbatch -J screen --wrap 'screen -D -m'
srun --jobid $(squeue -n screen -h -o '%A') --pty screen -rd

Ward

